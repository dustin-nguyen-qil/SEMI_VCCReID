{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "# Assuming you have a list 'frames_features' containing feature vectors for 8 frames\n",
    "frames_features = torch.randn((16, 8, 10)).cuda()\n",
    "frames_features_list = [list(frames_features[i]) for i in range(16)]\n",
    "# Number of frames\n",
    "num_frames = len(frames_features_list)\n",
    "\n",
    "# Number of classifiers (assuming 8 classifiers for 8 frames)\n",
    "num_classifiers = num_frames\n",
    "\n",
    "input_dim = 10\n",
    "num_classes_per_classifier = 150  #\n",
    "\n",
    "classifiers = [nn.Linear(input_dim, num_classes_per_classifier).cuda() for _ in range(num_classifiers)]\n",
    "\n",
    "batch_framewise_logits_list = []\n",
    "for batch_idx in range(16):\n",
    "    framewise_logits_list = []\n",
    "    for frame_idx in range(8):\n",
    "        frame_feature = frames_features[batch_idx][frame_idx]\n",
    "        classifier = classifiers[frame_idx]\n",
    "        # Forward pass through the classifier\n",
    "        logits = classifier(frame_feature)\n",
    "        # Store the logits for the current frame\n",
    "        framewise_logits_list.append(logits)\n",
    "    batch_framewise_logits_list.append(framewise_logits_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 150])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = torch.stack([torch.stack(batch_framewise_logits_list[i]) for i in range(len(batch_framewise_logits_list))])\n",
    "final_ = final.view(-1, 150)\n",
    "final_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = torch.randint(0, 150, (16,))\n",
    "class_indices = class_indices.repeat_interleave(8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0967, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(final_, class_indices)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15,\n",
       "        15, 15])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tensor = torch.arange(16)\n",
    "\n",
    "# Duplicate each element 7 more times to create a new tensor of size (128,)\n",
    "new_tensor = original_tensor.repeat_interleave(8)\n",
    "\n",
    "# Now the 'new_tensor' has a size of (128,)\n",
    "new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9770, 0.8260, 0.6012, 0.7240, 0.6263, 0.0738, 0.7951, 0.5541, 0.0893,\n",
       "        0.2656], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((16, 8, 10)).cuda()\n",
    "b = [list(a[i]) for i in range(a.size(0))]\n",
    "b[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((16, 8, 1024))\n",
    "b = a.permute(1, 0, 2)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/ccvid/gallery.pkl'\n",
    "\n",
    "with open(datapath, 'rb') as f:\n",
    "    content = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1\n",
    "len(content['data'][idx]['img_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_container = set()\n",
    "for item in content['data']:\n",
    "    pid_container.add(item['p_id'])\n",
    "\n",
    "pid_container = sorted(pid_container)\n",
    "pid2label = {pid:label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "for item in content['data']:\n",
    "    cur_pid = item['p_id']\n",
    "    new_pid = pid2label[cur_pid]\n",
    "    item['p_id'] = new_pid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath, 'wb') as f:\n",
    "    pickle.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in content['data']:\n",
    "    del item['tracklet_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datapath, 'wb') as f:\n",
    "    pickle.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "import os.path as osp\n",
    "\n",
    "old_path = '/media/dustin/DATA/Research/Video-based ReID'\n",
    "new_path = '/project/shah/dustin/VideoDatasets'\n",
    "\n",
    "# for dataset in os.listdir('data/vccr'):\n",
    "#     dataset_path = osp.join('data', dataset)\n",
    "# for file in os.listdir('data/ccvid'):\n",
    "#     mode = file.split('.')[0]\n",
    "file_path = 'data/ccvid/train.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    content = pickle.load(f)\n",
    "paths = []\n",
    "for item in content['data']:\n",
    "    for i in item['img_paths']:\n",
    "        path = i[0]\n",
    "        paths.append(path)\n",
    "    item['img_paths'] = paths\n",
    "# new_file_path = osp.join('data/ccvid', 'train_.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(content, f)\n",
    "    \n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
